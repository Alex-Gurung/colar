# VR-CLI-REWARD-MOD: Configuration for CoLaR with reference model rewards
seed: ~

model:
  target: src.models.colar_reference_reward.LitCoLaRReferenceReward
  model_kwargs:
    model_id: Qwen/Qwen2.5-1.5B-Instruct
    # model_id: DeepSeek-R1-Distill-Qwen-1.5B
    sft_method: colar
    chat_template: False

    do_lora: True
    lora_config:
      r: 128
      lora_alpha: 32

    latent_cot_config:
      ce_weight: 1
      embed_modeling_weight: 1
      embed_modeling_loss: mse  # {nll, mse}
      entropy_weight: 0
      pred_embed_forward_weight: 0
      max_compression_factor: 5
      pred_compressed_cot: True
      sqrt_mean: True

    latent_policy_config:
      lp_determinisitc: False
      lp_intermediate_size: 2048

    latent_generation_config:
      max_n_latent_forward: 64
      latent_temperature: 1.0
      compression_factor: 5

    answer_generation_config:
      max_new_tokens: 16
      do_sample: True
      top_p: 0.9
      temperature: 1.0

    # VR-CLI-REWARD-MOD: Reference model reward configuration
    reference_reward_config:
      enabled: True
      model_name: Qwen/Qwen2.5-1.5B-Instruct  # Can be different from main model
      reward_type: target_likelihood  # {target_likelihood, answer_quality, reasoning_quality}
      target_type: correct_answer     # {correct_answer, answer_format, reasoning_conclusion, custom}
      context_format: qa_format       # {simple, qa_format, conversation}
      reward_scale: 1.0               # Scale factor for rewards
      reward_clip: 5.0                # Clip rewards to [-clip, +clip]
      use_sigmoid: True               # Apply sigmoid to final reward
      device_map: cpu                 # {cpu, cuda:0, cuda:1, auto} - keep reference model on CPU to save GPU memory

      # VR-CLI-REWARD-MOD: Optional custom target for advanced usage
      custom_target: ""

      # VR-CLI-REWARD-MOD: Template for answer quality evaluation
      eval_prompt_template: |
        Question: {question}
        Proposed Answer: {pred_answer}
        Correct Answer: {gt_answer}

        Evaluate the quality of the proposed answer. Consider:
        1. Correctness of the final answer
        2. Quality of reasoning steps
        3. Clarity and completeness

        Rate on a scale of 0.0 to 1.0:

    do_rl: False
    rl_config:
      average_per_token_loss: False
      random_speed_in_group: False
      filter_dataset: False
      exp_batch_size: 8
      group_size: 8
      punish_latent_length: False
      clip_grad_norm: 1.0
      clip_eps: 0.2
      use_latent_loss: True
      use_answer_loss: True
      n_train_samples_per_epoch: 512  # better divisible to total_batch_size

  training_kwargs:
    optimizer:
      target: torch.optim.AdamW
      lr: 1e-4  # set to 1e-6 for rl
      weight_decay: 0.01
    use_scheduler: False
    scheduler:
      target: constant_schedule_with_warmup
      warmup_steps: 1000

trainer:
  max_epochs: 50

dataloader:
  batch_size: 256
  val_batch_size: 32
  num_workers: 8
  pin_memory: True
  persistent_workers: True